{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation - YAKE vs YAKE+ML Comparison\n",
    "\n",
    "This notebook evaluates the trained model and compares pure YAKE rankings with YAKE+ML combined rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "import joblib\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from ml.predict import load_latest_model, combine_scores\n",
    "\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n# Change to project root directory so relative paths work\nos.chdir('../..')\nprint(f\"Working directory: {os.getcwd()}\")\n\n# Load trained model\nmodel = load_latest_model()\nprint(f\"Model loaded: {model is not None}\")\n\nif model:\n    print(f\"Model type: {type(model).__name__}\")\n\n# Load feedback data\ndf = pd.read_csv('data/feedback.csv')\nprint(f\"Total samples: {len(df)}\")\n\n# Prepare features\nfeature_cols = ['length', 'yake_score', 'f1_wfreq', 'f2_wcase', 'f3_wpos', 'f4_wrel', 'f5_wspread']\nX = df[feature_cols].values\ny = df['label'].values\n\nprint(f\"Features shape: {X.shape}\")\nprint(f\"Labels distribution: {np.bincount(y)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions and probabilities\n",
    "y_pred = model.predict(X)\n",
    "y_proba = model.predict_proba(X)[:, 1]\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y, y_pred, target_names=['Rejected', 'Approved']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Rejected', 'Approved'], \n",
    "            yticklabels=['Rejected', 'Approved'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare YAKE vs YAKE+ML Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get combined scores with different alpha values\n",
    "yake_scores = df['yake_score'].values\n",
    "\n",
    "# Test different alpha values\n",
    "alphas = [0.5, 0.7, 0.9, 1.0]  # 1.0 = pure YAKE\n",
    "\n",
    "for alpha in alphas:\n",
    "    if alpha == 1.0:\n",
    "        final_scores = yake_scores\n",
    "        label = \"Pure YAKE\"\n",
    "    else:\n",
    "        final_scores = combine_scores(yake_scores, y_proba, alpha=alpha)\n",
    "        label = f\"Alpha={alpha}\"\n",
    "    \n",
    "    # Create a copy with rankings\n",
    "    df_temp = df.copy()\n",
    "    df_temp['final_score'] = final_scores\n",
    "    df_temp['rank'] = df_temp['final_score'].rank()\n",
    "    \n",
    "    # Calculate how many approved keywords are in top-k\n",
    "    top_k = 10\n",
    "    top_keywords = df_temp.nsmallest(top_k, 'final_score')\n",
    "    approved_in_top_k = top_keywords['label'].sum()\n",
    "    \n",
    "    print(f\"\\n{label}: {approved_in_top_k}/{top_k} approved keywords in top-{top_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking Quality Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ranking quality at different alpha values\n",
    "results = []\n",
    "\n",
    "for alpha in np.linspace(0, 1, 11):\n",
    "    if alpha == 1.0:\n",
    "        final_scores = yake_scores\n",
    "    else:\n",
    "        final_scores = combine_scores(yake_scores, y_proba, alpha=alpha)\n",
    "    \n",
    "    df_temp = df.copy()\n",
    "    df_temp['final_score'] = final_scores\n",
    "    \n",
    "    # Calculate precision@k for different k values\n",
    "    for k in [5, 10, 15, 20]:\n",
    "        top_k = df_temp.nsmallest(k, 'final_score')\n",
    "        precision = top_k['label'].sum() / k\n",
    "        results.append({\n",
    "            'alpha': alpha,\n",
    "            'k': k,\n",
    "            'precision': precision\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "for k in [5, 10, 15, 20]:\n",
    "    data = results_df[results_df['k'] == k]\n",
    "    plt.plot(data['alpha'], data['precision'], marker='o', label=f'P@{k}')\n",
    "\n",
    "plt.xlabel('Alpha (1.0 = Pure YAKE, 0.0 = Pure ML)')\n",
    "plt.ylabel('Precision@k')\n",
    "plt.title('Ranking Quality vs Alpha Parameter')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Rankings Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example of ranking differences\n",
    "alpha = 0.7\n",
    "final_scores = combine_scores(yake_scores, y_proba, alpha=alpha)\n",
    "\n",
    "comparison_df = df[['keyword', 'yake_score', 'label']].copy()\n",
    "comparison_df['ml_prob'] = y_proba\n",
    "comparison_df['final_score'] = final_scores\n",
    "\n",
    "# Top 10 by YAKE\n",
    "print(\"Top 10 by Pure YAKE:\")\n",
    "print(comparison_df.nsmallest(10, 'yake_score')[['keyword', 'yake_score', 'label']])\n",
    "\n",
    "# Top 10 by combined score\n",
    "print(\"\\nTop 10 by YAKE+ML (alpha=0.7):\")\n",
    "print(comparison_df.nsmallest(10, 'final_score')[['keyword', 'final_score', 'ml_prob', 'label']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}