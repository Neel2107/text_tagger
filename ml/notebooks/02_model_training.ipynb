{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training - Random Forest Keyword Classifier\n",
    "\n",
    "This notebook trains and evaluates a Random Forest model for keyword relevance classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nos.chdir('../..')\n\n# Load feedback data with deduplication\nfrom ml.train import load_feedback_data\n\nX, y, stats = load_feedback_data()\n\nprint(f\"Data preprocessing:\")\nprint(f\"  Original samples: {stats['original_samples']}\")\nprint(f\"  Duplicates removed: {stats['duplicates_removed']}\")\nprint(f\"  Conflicts dropped: {stats['conflicts_dropped']}\")\nprint(f\"  Clean samples: {stats['final_samples']}\")\n\nprint(f\"\\nApproval rate: {y.mean():.2%}\")\nprint(f\"Feature shape: {X.shape}\")\nprint(f\"Label distribution: {np.bincount(y)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest with default parameters from config\n",
    "from ml.config import MODEL_PARAMS\n",
    "\n",
    "model = RandomForestClassifier(**MODEL_PARAMS)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(model, X, y, cv=min(5, len(X)), scoring='accuracy')\n",
    "print(f\"Cross-validation accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=importance_df, x='importance', y='feature')\n",
    "plt.title('Feature Importance')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Hyperparameter Tuning\n\nTesting different RandomForest configurations to find optimal parameters."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Hyperparameter tuning with GridSearchCV\nprint(\"Running hyperparameter tuning (this may take a few minutes)...\")\n\nparam_grid = {\n    'n_estimators': [50, 100, 150, 200],\n    'max_depth': [5, 10, 15, 20, None],\n    'min_samples_split': [2, 5, 10, 15],\n    'min_samples_leaf': [1, 2, 4, 8]\n}\n\ngrid_search = GridSearchCV(\n    RandomForestClassifier(random_state=42),\n    param_grid,\n    cv=min(5, len(X)),\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=1\n)\n\ngrid_search.fit(X, y)\n\nprint(f\"\\nBest parameters: {grid_search.best_params_}\")\nprint(f\"Best CV score: {grid_search.best_score_:.2%}\")\nprint(f\"\\nComparison with default params:\")\nprint(f\"  Default (from config): {cv_scores.mean():.2%}\")\nprint(f\"  Tuned: {grid_search.best_score_:.2%}\")\nprint(f\"  Improvement: {(grid_search.best_score_ - cv_scores.mean()):.2%}\")\n\n# Use best model\nmodel = grid_search.best_estimator_"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Hyperparameter Tuning Results\n\nVisualizing how different parameters affect model performance."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize grid search results\nresults_df = pd.DataFrame(grid_search.cv_results_)\n\n# Plot: n_estimators vs score\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n# 1. n_estimators effect\nax = axes[0, 0]\nfor depth in [5, 10, 15, 20, None]:\n    mask = results_df['param_max_depth'] == depth\n    subset = results_df[mask].groupby('param_n_estimators')['mean_test_score'].mean()\n    ax.plot(subset.index, subset.values, marker='o', label=f'max_depth={depth}')\nax.set_xlabel('n_estimators')\nax.set_ylabel('CV Accuracy')\nax.set_title('Effect of n_estimators')\nax.legend()\nax.grid(alpha=0.3)\n\n# 2. max_depth effect\nax = axes[0, 1]\ndepth_scores = results_df.groupby('param_max_depth')['mean_test_score'].agg(['mean', 'std'])\nax.bar(range(len(depth_scores)), depth_scores['mean'], yerr=depth_scores['std'])\nax.set_xticks(range(len(depth_scores)))\nax.set_xticklabels([str(x) for x in depth_scores.index])\nax.set_xlabel('max_depth')\nax.set_ylabel('CV Accuracy')\nax.set_title('Effect of max_depth')\nax.grid(alpha=0.3, axis='y')\n\n# 3. min_samples_split effect\nax = axes[1, 0]\nsplit_scores = results_df.groupby('param_min_samples_split')['mean_test_score'].agg(['mean', 'std'])\nax.bar(range(len(split_scores)), split_scores['mean'], yerr=split_scores['std'])\nax.set_xticks(range(len(split_scores)))\nax.set_xticklabels(split_scores.index)\nax.set_xlabel('min_samples_split')\nax.set_ylabel('CV Accuracy')\nax.set_title('Effect of min_samples_split')\nax.grid(alpha=0.3, axis='y')\n\n# 4. min_samples_leaf effect\nax = axes[1, 1]\nleaf_scores = results_df.groupby('param_min_samples_leaf')['mean_test_score'].agg(['mean', 'std'])\nax.bar(range(len(leaf_scores)), leaf_scores['mean'], yerr=leaf_scores['std'])\nax.set_xticks(range(len(leaf_scores)))\nax.set_xticklabels(leaf_scores.index)\nax.set_xlabel('min_samples_leaf')\nax.set_ylabel('CV Accuracy')\nax.set_title('Effect of min_samples_leaf')\nax.grid(alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\n# Top 10 configurations\nprint(\"\\nTop 10 Parameter Combinations:\")\nprint(\"=\"*80)\ntop_10 = results_df.nsmallest(10, 'rank_test_score')[\n    ['param_n_estimators', 'param_max_depth', 'param_min_samples_split', \n     'param_min_samples_leaf', 'mean_test_score', 'std_test_score']\n]\nfor idx, row in top_10.iterrows():\n    print(f\"\\n{row['mean_test_score']:.2%} (Â±{row['std_test_score']:.2%})\")\n    print(f\"  n_estimators={row['param_n_estimators']}, max_depth={row['param_max_depth']}, \"\n          f\"min_samples_split={row['param_min_samples_split']}, min_samples_leaf={row['param_min_samples_leaf']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_path = f'../models/rf_model_{timestamp}.pkl'\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(f\"Number of samples: {len(X)}\")\n",
    "print(f\"CV Accuracy: {cv_scores.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create training summary\nsummary = {\n    'timestamp': timestamp,\n    'num_samples': len(X),\n    'num_features': X.shape[1],\n    'cv_accuracy': grid_search.best_score_,\n    'cv_std': results_df.loc[grid_search.best_index_, 'std_test_score'],\n    'best_params': grid_search.best_params_,\n    'improvement_over_default': grid_search.best_score_ - cv_scores.mean(),\n    'top_features': importance_df.head(3)['feature'].tolist()\n}\n\nprint(\"\\n=== Training Summary ===\")\nfor key, value in summary.items():\n    print(f\"{key}: {value}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"RECOMMENDED CONFIG UPDATE:\")\nprint(\"=\"*80)\nprint(\"\\nUpdate ml/config.py MODEL_PARAMS with:\")\nprint(f\"\"\"\nMODEL_PARAMS = {{\n    'n_estimators': {grid_search.best_params_['n_estimators']},\n    'max_depth': {grid_search.best_params_['max_depth']},\n    'min_samples_split': {grid_search.best_params_['min_samples_split']},\n    'min_samples_leaf': {grid_search.best_params_['min_samples_leaf']},\n    'random_state': 42\n}}\n\"\"\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}